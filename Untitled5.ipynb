{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfDacHqSu4p3ixfebJ7IKx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BastiCaa/prueba/blob/master/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm2oXZ5U5F4u",
        "colab_type": "text"
      },
      "source": [
        "### Bibliotecas necesarias para evitar errores y ademÃ¡s se lee el archivo excel \"dataset2.csv\" de un repositorio de github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORq2R5eNlZYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "faea2f99-44a9-47f6-ba16-4f93db875493"
      },
      "source": [
        "#Declaring necessary modules\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import logging\n",
        "os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '0'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)\n",
        "from datetime import datetime\n",
        "import math\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print (tf.__version__)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "tf.autograph.set_verbosity(2)\n",
        "\n",
        "#Pre-procesamiento de datos\n",
        "datos1 = pd.read_excel(\"https://raw.githubusercontent.com/BastiCaa/prueba/master/dataset1.xlsx\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt21UPgpjRID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d07e4a9b-b1a0-4f68-a516-a917c8f93556"
      },
      "source": [
        "\n",
        "print (datos1[['Longitud']])\n",
        "datos1.head()\n",
        "\n",
        "\n",
        "datos1.describe()\n",
        "\n",
        "crr_Nom_dummy = pd.get_dummies(datos1[\"crr_Nom\"], prefix=\"crr_Nom\")\n",
        "cluster_dummy = pd.get_dummies(datos1[\"cluster\"], prefix=\"cluster\")\n",
        "\n",
        "datos1 = pd.concat([datos1, crr_Nom_dummy], axis = 1)\n",
        "datos1 = pd.concat([datos1, cluster_dummy], axis = 1)\n",
        "\n",
        "datos1_dummies = datos1.drop(['crr_Nom', 'cluster'], axis = 1)\n",
        "\n",
        "# Inputs\n",
        "X = datos1_dummies.iloc[:,0:27].values\n",
        "\n",
        "\n",
        "# Target\n",
        "y = datos1_dummies.iloc[:,28:32].values\n",
        "\n",
        "# Division de la data de entrenamiento\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)#0.2\n",
        "\n",
        "###Escalar los datos\n",
        "escalar = StandardScaler()\n",
        "X_train = escalar.fit_transform(X_train)\n",
        "\n",
        "X_test = escalar.transform(X_test)\n",
        "\n",
        "class Model (object):\n",
        "    def __init__(self):\n",
        "    # In practice, these should be initialized to random values (for example, with `tf.random.normal`)\n",
        "        samples, features = (6292,27) #x.shape\n",
        "        hidden1_nodes = 27\n",
        "        hidden2_nodes = 4\n",
        "        self.theta1 = tf.Variable(tf.random.normal([features+1,hidden1_nodes], dtype= tf.float64) ,name = \"Theta1\")\n",
        "        self.theta2 = tf.Variable(tf.random.normal([hidden1_nodes+1,hidden2_nodes], dtype= tf.float64), name = \"Theta2\")\n",
        "    \n",
        "    def __call__(self, X_train): # Son metodos que vienen predifinidos desde el object.\n",
        "        # Estructura del dibujo del cuaderno.\n",
        "        bias1 = tf.constant(1, shape=(6292,1), dtype=tf.float64, name='bias1')#6292\n",
        "        bias2 = tf.constant(1, shape=(6292,1), dtype=tf.float64, name='bias2')\n",
        "        a0 = tf.concat([bias1,X_train],1, name='a0')\n",
        "        z1 = tf.matmul(a0,self.theta1, name='z1') \n",
        "        a1 = tf.concat([bias2,tf.sigmoid(z1)],1,name='a1')\n",
        "        z2 = tf.matmul(a1,self.theta2, name='z2')\n",
        "        a2 = tf.nn.softmax(z2, name='a2')\n",
        "        return a2 # Respuesta, aqui ya predice\n",
        "\n",
        "\n",
        "def loss(target_y, predicted_y): # Funcion de divergencia, Aqui esta la funcion Logistic_regression\n",
        "    losses=tf.keras.losses.categorical_crossentropy(target_y,predicted_y) #entrega un array de longitud mayor a 1\n",
        "    regulacion=0.001*tf.reduce_sum(tf.abs(target_y - predicted_y)) \n",
        "    losses_1=tf.reduce_mean(losses)+regulacion  #entrega el promedio del array losses el cual es de una dimension, esto para evitar el lenght-1\n",
        "    return losses_1, regulacion\n",
        "\n",
        "def train(model, inputs, outputs, learning_rate): # Recibe todo, para entrenar el modelo.\n",
        "    with tf.GradientTape() as t:\n",
        "        current_loss =loss(outputs, model(inputs)) # El model con el call sabe predecir\n",
        "    dThe1, dThe2 = t.gradient(current_loss[0], [model.theta1, model.theta2])\n",
        "    model.theta1.assign_sub(learning_rate * dThe1)\n",
        "    model.theta2.assign_sub(learning_rate * dThe2)\n",
        "    \n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"\n",
        "    Returns a custom learning rate that decreases as epochs progress.\n",
        "    \"\"\"\n",
        "    learning_rate = 0.3\n",
        "    if epoch > 5000:\n",
        "        learning_rate = 0.2\n",
        "    if epoch > 10000:\n",
        "        learning_rate = 0.1\n",
        "    if epoch > 20000:\n",
        "        learning_rate = 0.05\n",
        "    return learning_rate\n",
        "\n",
        "def matriz(y_predicho,y_test):\n",
        "    y_test_prec=np.argmax(y_predicho,axis=1)\n",
        "    y_compro=np.argmax(y_test,axis=1)\n",
        "    matriz=confusion_matrix(y_compro,y_test_prec)\n",
        "    print(matriz)\n",
        "\n",
        "def evaluar(X_test,y_test,theta1_t,theta2_t,batch_size):\n",
        "    bias1_t= tf.constant(1, shape=(1574,1), dtype=tf.float64, name='bias1')#6292\n",
        "    bias2_t= tf.constant(1, shape=(1574,1), dtype=tf.float64, name='bias2')\n",
        "    a0_t= tf.concat([bias1_t,X_test],1, name='a0_t')\n",
        "    z1_t= tf.matmul(a0_t,theta1_t, name='z1_t') \n",
        "    a1_t= tf.concat([bias2_t,tf.sigmoid(z1_t)],1,name='a1_t')\n",
        "    z2_t= tf.matmul(a1_t,theta2_t, name='z2_t')\n",
        "    a2_t= tf.nn.softmax(z2_t, name='a2_t')\n",
        "    return a2_t\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Longitud\n",
            "0    -73.16487\n",
            "1    -73.16487\n",
            "2    -73.16487\n",
            "3    -73.16487\n",
            "4    -73.16487\n",
            "...        ...\n",
            "7861 -73.01875\n",
            "7862 -73.01875\n",
            "7863 -73.01875\n",
            "7864 -73.01875\n",
            "7865 -73.01875\n",
            "\n",
            "[7866 rows x 1 columns]\n",
            "Epoch  0: learning_rate=0.30000, loss=13.75971 , regulacion=9.79019\n",
            "Epoch 500: learning_rate=0.30000, loss=2.43569 , regulacion=2.03754\n",
            "Epoch 1000: learning_rate=0.30000, loss=2.06079 , regulacion=1.69699\n",
            "Epoch 1500: learning_rate=0.30000, loss=1.84326 , regulacion=1.50266\n",
            "Epoch 2000: learning_rate=0.30000, loss=1.68397 , regulacion=1.36491\n",
            "Epoch 2500: learning_rate=0.30000, loss=1.55719 , regulacion=1.25416\n",
            "Epoch 3000: learning_rate=0.30000, loss=1.45065 , regulacion=1.15824\n",
            "Epoch 3500: learning_rate=0.30000, loss=1.38471 , regulacion=1.09640\n",
            "Epoch 4000: learning_rate=0.30000, loss=1.33056 , regulacion=1.04663\n",
            "Epoch 4500: learning_rate=0.30000, loss=1.28213 , regulacion=1.00300\n",
            "Epoch 5000: learning_rate=0.30000, loss=1.23404 , regulacion=0.96114\n",
            "Epoch 5500: learning_rate=0.20000, loss=1.20490 , regulacion=0.93495\n",
            "Epoch 6000: learning_rate=0.20000, loss=1.18150 , regulacion=0.91353\n",
            "Epoch 6500: learning_rate=0.20000, loss=1.16043 , regulacion=0.89447\n",
            "Epoch 7000: learning_rate=0.20000, loss=1.13710 , regulacion=0.87380\n",
            "Epoch 7500: learning_rate=0.20000, loss=1.11641 , regulacion=0.85606\n",
            "Epoch 8000: learning_rate=0.20000, loss=1.09821 , regulacion=0.84007\n",
            "Epoch 8500: learning_rate=0.20000, loss=1.08183 , regulacion=0.82544\n",
            "Epoch 9000: learning_rate=0.20000, loss=1.06613 , regulacion=0.81156\n",
            "Epoch 9500: learning_rate=0.20000, loss=1.05031 , regulacion=0.79796\n",
            "Epoch 10000: learning_rate=0.20000, loss=1.03338 , regulacion=0.78299\n",
            "Epoch 10500: learning_rate=0.10000, loss=1.02402 , regulacion=0.77468\n",
            "Epoch 11000: learning_rate=0.10000, loss=1.01546 , regulacion=0.76695\n",
            "Epoch 11500: learning_rate=0.10000, loss=1.00661 , regulacion=0.75923\n",
            "Epoch 12000: learning_rate=0.10000, loss=0.99788 , regulacion=0.75158\n",
            "Epoch 12500: learning_rate=0.10000, loss=0.98996 , regulacion=0.74449\n",
            "Epoch 13000: learning_rate=0.10000, loss=0.98276 , regulacion=0.73802\n",
            "Epoch 13500: learning_rate=0.10000, loss=0.97562 , regulacion=0.73170\n",
            "Epoch 14000: learning_rate=0.10000, loss=0.96807 , regulacion=0.72495\n",
            "Epoch 14500: learning_rate=0.10000, loss=0.96127 , regulacion=0.71862\n",
            "Epoch 15000: learning_rate=0.10000, loss=0.95518 , regulacion=0.71296\n",
            "Epoch 15500: learning_rate=0.10000, loss=0.94937 , regulacion=0.70755\n",
            "Epoch 16000: learning_rate=0.10000, loss=0.94386 , regulacion=0.70232\n",
            "Epoch 16500: learning_rate=0.10000, loss=0.93863 , regulacion=0.69727\n",
            "Epoch 17000: learning_rate=0.10000, loss=0.93358 , regulacion=0.69231\n",
            "Epoch 17500: learning_rate=0.10000, loss=0.92867 , regulacion=0.68743\n",
            "Epoch 18000: learning_rate=0.10000, loss=0.92386 , regulacion=0.68268\n",
            "Epoch 18500: learning_rate=0.10000, loss=0.91893 , regulacion=0.67787\n",
            "Epoch 19000: learning_rate=0.10000, loss=0.91388 , regulacion=0.67301\n",
            "Epoch 19500: learning_rate=0.10000, loss=0.90903 , regulacion=0.66840\n",
            "[[1025   36    0    0]\n",
            " [  24 2839   34   11]\n",
            " [   0   41 1352   23]\n",
            " [   0   13   23  871]]\n",
            "[[233  38   0   0]\n",
            " [ 31 643  33  16]\n",
            " [  0  44 293  20]\n",
            " [  0  11  22 190]]\n",
            "Presiciones-> C1= 0.97156 C2=0.97276 C3=0.95717  C4=0.96137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOuoiM4M6AaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model() # Voy ajustando mis tetas en el proceso de aprendizaje\n",
        "theta1_hist, theta2_hist = [] , []\n",
        "for epoch in range(20000):\n",
        "    learning_rate = lr_schedule(epoch)\n",
        "    current_loss = loss(y_train, model(X_train))\n",
        "    train(model, X_train, y_train, learning_rate=learning_rate)\n",
        "    if epoch % 500 == 0:\n",
        "        theta1_hist.append(model.theta1.numpy())\n",
        "        theta2_hist.append(model.theta2.numpy())\n",
        "        print('Epoch %2d: learning_rate=%2.5f, loss=%2.5f , regulacion=%2.5f' % (epoch, learning_rate, current_loss[0],current_loss[1]))\n",
        "\n",
        "\n",
        "##Evaluacion de datos\n",
        "theta1_t=model.theta1\n",
        "theta2_t=model.theta2\n",
        "batch_size=1574\n",
        "#testea con funcion evaluar\n",
        "testeo=evaluar(X_test,y_test,theta1_t,theta2_t,batch_size)\n",
        "##creacion de matrices\n",
        "X_entrenado=model(X_train)\n",
        "#matriz de confusion de muestras entrenadas\n",
        "matriz_train=matriz(X_entrenado,y_train)\n",
        "#matriz de confusion de muestras de testeo\n",
        "matriz_testeo=matriz(testeo,y_test)\n",
        "\n",
        "#prueba F1-score\n",
        "y_true=np.argmax(y_train,axis=1)\n",
        "y_predicho=np.argmax(model(X_train),axis=1)\n",
        "significancia=f1_score(y_true,y_predicho,average=None)\n",
        "print('Presiciones-> C1= %2.5f C2=%2.5f C3=%2.5f  C4=%2.5f'% (significancia[0],significancia[1],significancia[2],significancia[3]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}