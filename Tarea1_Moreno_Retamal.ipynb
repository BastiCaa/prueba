{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea1_Moreno_Retamal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BastiCaa/prueba/blob/master/Tarea1_Moreno_Retamal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm2oXZ5U5F4u",
        "colab_type": "text"
      },
      "source": [
        "# Ejercicio 1:\n",
        "El set de datos 1 contiene información respecto a los estudiantes que han ingresado a la facultad de ingeniería. Se le pide que implemente una regresión logística que permita aprender a clasificar a que clúster pertenece cada alumno de acuerdo la información que se entrega. Recuerde que por convención la última columna del set de datos representa la señal que usted quiere aprender. También tenga en cuenta que las clases están desbalanceadas y que se espera que usted utilice 80% de los datos para entrenar y el 20% restante para testear. Además, presente una matriz de confusión y el f1-score. Finalmente, una restricción fuerte de este ejercicio es que todo debe estar programado en Tensorflow 2 y desarrollado en Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLsWDn7rVCXw",
        "colab_type": "text"
      },
      "source": [
        "## Preparación del codigo\n",
        "En esta primera celda, cargaremos las bibliotecas necesarias para correr el codigo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORq2R5eNlZYP",
        "colab_type": "code",
        "outputId": "367340e7-e79c-496f-e635-b1f8c405379a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import logging\n",
        "os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '0'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)\n",
        "\n",
        "import math\n",
        "import statistics as stats\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print (tf.__version__)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "tf.autograph.set_verbosity(2)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K5zQsny-bBT",
        "colab_type": "text"
      },
      "source": [
        "A continuación, cargaremos nuestro conjunto de datos de un repositorio de github."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM6KhDOz-LJF",
        "colab_type": "code",
        "outputId": "52bcfae8-4213-4aa6-d966-5927884eb459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "datos1 = pd.read_excel(\"https://raw.githubusercontent.com/BastiCaa/prueba/master/dataset1.xlsx\")\n",
        "print (datos1[['Longitud']])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Longitud\n",
            "0    -73.16487\n",
            "1    -73.16487\n",
            "2    -73.16487\n",
            "3    -73.16487\n",
            "4    -73.16487\n",
            "...        ...\n",
            "7861 -73.01875\n",
            "7862 -73.01875\n",
            "7863 -73.01875\n",
            "7864 -73.01875\n",
            "7865 -73.01875\n",
            "\n",
            "[7866 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUL-2UjQ_PoL",
        "colab_type": "text"
      },
      "source": [
        "## Análisis de los datos\n",
        "Familiarización de los datos imprimiendo la estadistica descriptiva de los mismos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVHxhiri_NGv",
        "colab_type": "code",
        "outputId": "7d05d1e5-41cd-458b-cb79-96cc82a9b4f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "datos1.head()\n",
        "datos1.describe()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>psu_Nem</th>\n",
              "      <th>psu_Leng</th>\n",
              "      <th>psu_Mate</th>\n",
              "      <th>psu_Cie</th>\n",
              "      <th>psu_Pond</th>\n",
              "      <th>Longitud</th>\n",
              "      <th>Latitud</th>\n",
              "      <th>cred_apr</th>\n",
              "      <th>distancia</th>\n",
              "      <th>F</th>\n",
              "      <th>M</th>\n",
              "      <th>PARTICULAR_SUBVENCIONADO</th>\n",
              "      <th>PARTICULAR_PAGADO</th>\n",
              "      <th>MUNICIPAL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>7866.000000</td>\n",
              "      <td>7866.000000</td>\n",
              "      <td>7866.000000</td>\n",
              "      <td>7866.000000</td>\n",
              "      <td>7866.000000</td>\n",
              "      <td>7866.000000</td>\n",
              "      <td>7866.000000</td>\n",
              "      <td>7866.000000</td>\n",
              "      <td>7866.000000</td>\n",
              "      <td>7866.000000</td>\n",
              "      <td>7866.000000</td>\n",
              "      <td>7866.000000</td>\n",
              "      <td>7866.000000</td>\n",
              "      <td>7866.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>668.634630</td>\n",
              "      <td>618.109586</td>\n",
              "      <td>669.857107</td>\n",
              "      <td>622.594839</td>\n",
              "      <td>659.579456</td>\n",
              "      <td>-72.846972</td>\n",
              "      <td>-36.899123</td>\n",
              "      <td>18.208874</td>\n",
              "      <td>33.065784</td>\n",
              "      <td>0.220061</td>\n",
              "      <td>0.779939</td>\n",
              "      <td>0.521103</td>\n",
              "      <td>0.241927</td>\n",
              "      <td>0.236969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>71.116963</td>\n",
              "      <td>65.337494</td>\n",
              "      <td>56.908442</td>\n",
              "      <td>60.070238</td>\n",
              "      <td>48.014462</td>\n",
              "      <td>0.404343</td>\n",
              "      <td>0.282372</td>\n",
              "      <td>14.321758</td>\n",
              "      <td>39.204470</td>\n",
              "      <td>0.414314</td>\n",
              "      <td>0.414314</td>\n",
              "      <td>0.499586</td>\n",
              "      <td>0.428278</td>\n",
              "      <td>0.425250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>373.000000</td>\n",
              "      <td>322.000000</td>\n",
              "      <td>494.000000</td>\n",
              "      <td>322.000000</td>\n",
              "      <td>512.000000</td>\n",
              "      <td>-73.649141</td>\n",
              "      <td>-38.016235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.415859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>620.000000</td>\n",
              "      <td>574.000000</td>\n",
              "      <td>630.000000</td>\n",
              "      <td>585.000000</td>\n",
              "      <td>626.000000</td>\n",
              "      <td>-73.097558</td>\n",
              "      <td>-36.902500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.273443</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>678.000000</td>\n",
              "      <td>616.000000</td>\n",
              "      <td>666.000000</td>\n",
              "      <td>623.000000</td>\n",
              "      <td>657.000000</td>\n",
              "      <td>-73.046415</td>\n",
              "      <td>-36.828456</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>8.198763</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>723.000000</td>\n",
              "      <td>660.000000</td>\n",
              "      <td>702.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>692.000000</td>\n",
              "      <td>-72.541530</td>\n",
              "      <td>-36.778573</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>85.512871</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>826.000000</td>\n",
              "      <td>831.000000</td>\n",
              "      <td>850.000000</td>\n",
              "      <td>850.000000</td>\n",
              "      <td>825.000000</td>\n",
              "      <td>-71.549363</td>\n",
              "      <td>-36.132387</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>136.283837</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           psu_Nem     psu_Leng  ...  PARTICULAR_PAGADO    MUNICIPAL\n",
              "count  7866.000000  7866.000000  ...        7866.000000  7866.000000\n",
              "mean    668.634630   618.109586  ...           0.241927     0.236969\n",
              "std      71.116963    65.337494  ...           0.428278     0.425250\n",
              "min     373.000000   322.000000  ...           0.000000     0.000000\n",
              "25%     620.000000   574.000000  ...           0.000000     0.000000\n",
              "50%     678.000000   616.000000  ...           0.000000     0.000000\n",
              "75%     723.000000   660.000000  ...           0.000000     0.000000\n",
              "max     826.000000   831.000000  ...           1.000000     1.000000\n",
              "\n",
              "[8 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8NqfF-hAQT0",
        "colab_type": "text"
      },
      "source": [
        "## Pre-procesamiento de los datos\n",
        "Como dentro de la base de datos nos encontramos con datos tipos string, es necesario hacerles un one hot encoding a los datos de estas columnas. De esta manera podremos trabajar con ellos de la forma en que se desea. Además, se definieron las entradas y salida de los datos, junto con la división del 80% de los datos para entrenar y el 20% restante para testear. Por último, se escalaron los datos, ya que las caracteristicas de los mismos son completamente distintas en magnitudes, unidades y rango, para asi llevarlos a un mismo nivel de magnitudes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMty9lyBAPxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crr_Nom_dummy = pd.get_dummies(datos1[\"crr_Nom\"], prefix=\"crr_Nom\")\n",
        "cluster_dummy = pd.get_dummies(datos1[\"cluster\"], prefix=\"cluster\")\n",
        "\n",
        "datos1 = pd.concat([datos1, crr_Nom_dummy], axis = 1)\n",
        "datos1 = pd.concat([datos1, cluster_dummy], axis = 1)\n",
        "\n",
        "datos1_dummies = datos1.drop(['crr_Nom', 'cluster'], axis = 1)\n",
        "\n",
        "# Inputs\n",
        "X = datos1_dummies.iloc[:,0:27].values\n",
        "\n",
        "# Target\n",
        "y = datos1_dummies.iloc[:,28:32].values\n",
        "\n",
        "# Division de la data de entrenamiento\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)#0.2\n",
        "\n",
        "# Escalación de los datos\n",
        "escalar = StandardScaler()\n",
        "X_train = escalar.fit_transform(X_train)\n",
        "\n",
        "X_test = escalar.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tTAKizTAgxK",
        "colab_type": "text"
      },
      "source": [
        "## Definición de funciones\n",
        "\n",
        "En esta parte se definen las funciones necesarias que requiere el modelo para el entrenamiento de la red neuronal armada en Model(object). La funcion de activación que se utlizo en la primera capa fue la sigmoid y en la segunda capa fue la softmax, ya que esta es la empleada en métodos de clasificación multiclase. También, se utilizó la función de divergencia crossentropy junto con la regularización L1 para calcular el error del modelo. Por ultimo, para la optimizacion del modelo, se uso el GradientTape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1doGRsjzAhFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model (object):\n",
        "    def __init__(self):\n",
        "        samples, features = (6292,27) #x.shape\n",
        "        hidden1_nodes = 27\n",
        "        hidden2_nodes = 4\n",
        "        self.theta1 = tf.Variable(tf.random.normal([features+1,hidden1_nodes], dtype= tf.float64) ,name = \"Theta1\")\n",
        "        self.theta2 = tf.Variable(tf.random.normal([hidden1_nodes+1,hidden2_nodes], dtype= tf.float64), name = \"Theta2\")\n",
        "    \n",
        "    def __call__(self, X_train): # Son metodos que vienen predifinidos desde el object.\n",
        "        bias1 = tf.constant(1, shape=(6292,1), dtype=tf.float64, name='bias1')\n",
        "        bias2 = tf.constant(1, shape=(6292,1), dtype=tf.float64, name='bias2')\n",
        "        a0 = tf.concat([bias1,X_train],1, name='a0')\n",
        "        z1 = tf.matmul(a0,self.theta1, name='z1') \n",
        "        a1 = tf.concat([bias2,tf.sigmoid(z1)],1,name='a1')\n",
        "        z2 = tf.matmul(a1,self.theta2, name='z2')\n",
        "        a2 = tf.nn.softmax(z2, name='a2')\n",
        "        return a2 # Respuesta, aqui ya predice\n",
        "\n",
        "\n",
        "def loss(target_y, predicted_y): # Funcion de divergencia.\n",
        "    losses = tf.keras.losses.categorical_crossentropy(target_y,predicted_y) #entrega un array de longitud mayor a 1\n",
        "    regulacion = 0.001*tf.reduce_sum(tf.abs(target_y - predicted_y)) \n",
        "    losses_1 = tf.reduce_mean(losses) + regulacion  #entrega el promedio del array losses el cual es de una dimension, esto para evitar el lenght-1\n",
        "    return losses_1, regulacion\n",
        "\n",
        "def train(model, inputs, outputs, learning_rate): # Recibe todo, para entrenar el modelo.\n",
        "    with tf.GradientTape() as t:\n",
        "        current_loss =loss(outputs, model(inputs)) # El model con el call sabe predecir\n",
        "    dThe1, dThe2 = t.gradient(current_loss[0], [model.theta1, model.theta2])\n",
        "    model.theta1.assign_sub(learning_rate * dThe1)\n",
        "    model.theta2.assign_sub(learning_rate * dThe2)\n",
        "    \n",
        "def lr_schedule(epoch):\n",
        "    learning_rate = 0.5\n",
        "    if epoch > 5000:\n",
        "        learning_rate = 0.4\n",
        "    if epoch > 7000:\n",
        "        learning_rate = 0.2\n",
        "    #if epoch > 10000:\n",
        "        #learning_rate = 0.05\n",
        "    return learning_rate\n",
        "\n",
        "def matriz(y_predicho, y_test):\n",
        "    y_test_prec = np.argmax(y_predicho,axis=1)#Se transforma el vector Encode a uno con la posicion del mayor elemento.\n",
        "    y_compro = np.argmax(y_test,axis=1)\n",
        "    matriz = confusion_matrix(y_compro, y_test_prec)#Se crea la matriz de confusion de dimension (4,4) para los 4 cluster disponibles\n",
        "    print(matriz)\n",
        "    return matriz\n",
        "\n",
        "def evaluar(X_test, y_test, theta1_t, theta2_t, batch_size): #Se toman los thetas obtenidos en el entrenamiento y ocupan para predecir la muestra a testear\n",
        "    bias1_t = tf.constant(1, shape=(1574,1), dtype=tf.float64, name='bias1')\n",
        "    bias2_t = tf.constant(1, shape=(1574,1), dtype=tf.float64, name='bias2')\n",
        "    a0_t = tf.concat([bias1_t, X_test],1, name='a0_t')\n",
        "    z1_t = tf.matmul(a0_t, theta1_t, name='z1_t') \n",
        "    a1_t = tf.concat([bias2_t, tf.sigmoid(z1_t)],1, name='a1_t')\n",
        "    z2_t = tf.matmul(a1_t, theta2_t, name='z2_t')\n",
        "    a2_t = tf.nn.softmax(z2_t, name='a2_t')\n",
        "    return a2_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppRgS2I5A7a4",
        "colab_type": "text"
      },
      "source": [
        "## Entrenamiento del modelo\n",
        "Ahora podemos llamar a las funciones para entrenar el modelo. Entrenaremos el modelo en un rango 10000 epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOuoiM4M6AaY",
        "colab_type": "code",
        "outputId": "a3d250ff-9ac0-4150-9349-eca6c96772f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "model = Model() # Voy ajustando mis thetas en el proceso de aprendizaje\n",
        "theta1_hist, theta2_hist = [] , []\n",
        "for epoch in range(10000):\n",
        "    learning_rate = lr_schedule(epoch)\n",
        "    current_loss = loss(y_train, model(X_train))\n",
        "    train(model, X_train, y_train, learning_rate=learning_rate)\n",
        "    if epoch % 1000 == 0:\n",
        "        theta1_hist.append(model.theta1.numpy())\n",
        "        theta2_hist.append(model.theta2.numpy())\n",
        "        print('Epoch %2d: learning_rate=%2.5f, loss=%2.5f , regulacion=%2.5f' % (epoch, learning_rate, current_loss[0],current_loss[1]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0: learning_rate=0.50000, loss=10.24522 , regulacion=7.09497\n",
            "Epoch 1000: learning_rate=0.50000, loss=1.65963 , regulacion=1.35423\n",
            "Epoch 2000: learning_rate=0.50000, loss=1.32642 , regulacion=1.05019\n",
            "Epoch 3000: learning_rate=0.50000, loss=1.18340 , regulacion=0.90997\n",
            "Epoch 4000: learning_rate=0.50000, loss=1.09253 , regulacion=0.81907\n",
            "Epoch 5000: learning_rate=0.50000, loss=1.02659 , regulacion=0.75530\n",
            "Epoch 6000: learning_rate=0.40000, loss=0.99035 , regulacion=0.72030\n",
            "Epoch 7000: learning_rate=0.40000, loss=0.96157 , regulacion=0.69259\n",
            "Epoch 8000: learning_rate=0.20000, loss=0.94860 , regulacion=0.68083\n",
            "Epoch 9000: learning_rate=0.20000, loss=0.93475 , regulacion=0.66893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2_pMDxdGP0v",
        "colab_type": "text"
      },
      "source": [
        "## Evaluación del modelo\n",
        "Finalmente, se realizaron predicciones sobre los datos de entrenamiento para ver qué tan bien los ajustó nuestro modelo durante el entrenamiento. Junto a esto, se midió qué tan bien el modelo realiza generalizaciones con respecto a los datos nuevos. Y ademas, se calcularon las metricas respectivas para verificar el rendimiento y precisión del modelo. Presentando una matriz de confusión y el f1-score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6XgyPnmGQHc",
        "colab_type": "code",
        "outputId": "438d028a-c21d-4188-e241-62ff0ad9f4cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#Evaluacion de datos\n",
        "theta1_t = model.theta1\n",
        "theta2_t = model.theta2\n",
        "batch_size = 1574\n",
        "\n",
        "#Testea con funcion evaluar\n",
        "testeo = evaluar(X_test,y_test,theta1_t,theta2_t,batch_size)\n",
        "\n",
        "#Creacion de matrices\n",
        "X_entrenado = model(X_train)\n",
        "\n",
        "#Matriz de confusion de muestras entrenadas\n",
        "print('Matriz de confusion de muestras entrenadas')\n",
        "matriz_train = matriz(X_entrenado,y_train)\n",
        "\n",
        "#Matriz de confusion de muestras de testeo\n",
        "print('Matriz de confusion de muestras de testeo')\n",
        "matriz_testeo = matriz(testeo,y_test)\n",
        "\n",
        "#Prueba F1-score\n",
        "y_true = np.argmax(y_train, axis=1)\n",
        "y_predicho = np.argmax(model(X_train),axis=1)\n",
        "y_testeo = np.argmax(testeo, axis=1)\n",
        "y_test1 = np.argmax(y_test, axis=1)\n",
        "significancia_train = f1_score(y_true, y_predicho, average=None)\n",
        "significancia_test = f1_score(y_test1, y_testeo, average=None)\n",
        "mean_train=stats.mean(significancia_train)\n",
        "mean_test=stats.mean(significancia_test)\n",
        "print('Precision entrenamiento por Cluster: C1=%2.5f C2=%2.5f C3=%2.5f C4=%2.5f'% (significancia_train[0],significancia_train[1],significancia_train[2],significancia_train[3]))\n",
        "print('Precision testeo por Cluster: C1=%2.5f C2=%2.5f C3=%2.5f C4=%2.5f'% (significancia_test[0],significancia_test[1],significancia_test[2],significancia_test[3]))\n",
        "print('Precision promedio de entrenamiento: %2.5f' % mean_train)\n",
        "print('Precision promedio de entrenamiento: %2.5f' % mean_test)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matriz de confusion de muestras entrenadas\n",
            "[[1029   36    0    0]\n",
            " [  18 2853   34   12]\n",
            " [   0   32 1350   34]\n",
            " [   0   14   32  848]]\n",
            "Matriz de confusion de muestras de testeo\n",
            "[[232  35   0   0]\n",
            " [ 37 630  36  11]\n",
            " [  0  49 279  29]\n",
            " [  0  16  38 182]]\n",
            "Precision entrenamiento por Cluster: C1=0.97443 C2=0.97505 C3=0.95339 C4=0.94855\n",
            "Precision testeo por Cluster: C1=0.86567 C2=0.87258 C3=0.78592 C4=0.79476\n",
            "Precision promedio de entrenamiento: 0.96285\n",
            "Precision promedio de entrenamiento: 0.82973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xB1cxBEVvEf",
        "colab_type": "text"
      },
      "source": [
        "# Ejercicio 2:\n",
        "El set de datos 2 contiene información respecto a zonas de exclusión de pesca en las costas de la octava región. Se le solicita a usted desarrollar un modelo de clasificación con el cual poder predecir si un barco se encuentra o no en la zona de exclusión. Nuevamente considere que las clases están desbalanceadas y que se espera que usted utilice 80% de los datos para entrenar y el 20% restante para testear. Además, presente una matriz de confusión y el f1-score. Finalmente, una restricción fuerte de este ejercicio es que todo debe estar programado en Tensorflow 2 y desarrollado el mismo Colab notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgAzQI67WQrR",
        "colab_type": "text"
      },
      "source": [
        "## Preparación del codigo\n",
        "Para este problema se usaran las mismas bibliotecas cargadas en el ejercicio 1 para correr el codigo. A continuación, cargaremos nuestro conjunto de datos de un drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ult09wpTWVWo",
        "colab_type": "code",
        "outputId": "3a895425-62b7-4b98-f25d-2bb755ddea8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from pydrive.drive import GoogleDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "myfile = drive.CreateFile({'id': '1ZLZA1oNBgVVN6hBwkW0eLUd4gCluCucL'})\n",
        "myfile.GetContentFile('dataset2.csv')\n",
        "\n",
        "datos2 = pd.read_csv(\"dataset2.csv\", sep=\",\")\n",
        "print(datos2.describe())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   x             y     condition\n",
            "count   81647.000000  8.164700e+04  81647.000000\n",
            "mean    -3086.777977  5.838926e+06      0.196713\n",
            "std     63564.806223  1.116634e+05      0.397516\n",
            "min   -115137.483721  5.641731e+06      0.000000\n",
            "25%    -57057.230344  5.740942e+06      0.000000\n",
            "50%     -3881.231445  5.841880e+06      0.000000\n",
            "75%     48810.641757  5.938679e+06      0.000000\n",
            "max    142472.558864  6.025505e+06      1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjuJ1JYVWVxI",
        "colab_type": "text"
      },
      "source": [
        "## Pre-procesamiento de los datos\n",
        "Lo primero que se realizo fue pasar los datos 'y' y 'condition' a float64, ya que se encontraban en int64. Además, se definieron las entradas y salida de los datos, junto con la división del 80% de los datos para entrenar y el 20% restante para testear. Por último, se escalaron los datos, ya que las caracteristicas de los mismos son completamente distintas en magnitudes, unidades y rango, para asi llevarlos a un mismo nivel de magnitudes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EblhwdWvWWDZ",
        "colab_type": "code",
        "outputId": "38361cb2-28e7-46c0-8184-2562c1b4e7ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Pasando de int64 a float64\n",
        "datos2['y'] = datos2.y.astype(float)\n",
        "datos2['condition'] = datos2.condition.astype(float)\n",
        "\n",
        "# Declare the inputs\n",
        "x2 = datos2.drop('condition', axis=1).values\n",
        "\n",
        "# Target\n",
        "y2 = datos2[['condition']].values\n",
        "\n",
        "\n",
        "# Division de la data de entrenamiento\n",
        "x_train2, x_test2, y_train2, y_test2 = train_test_split(x2, y2, test_size=0.2)\n",
        "\n",
        "# Escalar los datos\n",
        "escalar2 = StandardScaler()\n",
        "x_train2 = escalar2.fit_transform(x_train21)\n",
        "x_test2 = escalar2.transform(x_test21)\n",
        "print(tf.shape(x_train2))\n",
        "print(tf.shape(y_train2))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([65317     2], shape=(2,), dtype=int32)\n",
            "tf.Tensor([65317     1], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ghzlPdxWWS1",
        "colab_type": "text"
      },
      "source": [
        "## Definición de funciones\n",
        "\n",
        "En esta parte se definen las funciones necesarias que requiere el modelo para el entrenamiento de la red neuronal armada en Model(object). La función de activacion que se utilizo fue la sigmoid en las 3 capas. También, se utilizó la función de divergencia de Logistic Regression junto con la regularización L1 para calcular el error del modelo. Por ultimo, para la optimizacion del modelo, se uso el GradientTape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT9HwDUbWWfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model2(object):\n",
        "    def __init__(self):\n",
        "        samples2, features2 = (81647,2) #x.shape\n",
        "        hidden1_nodes2 = 9\n",
        "        hidden2_nodes2 = 2\n",
        "        hidden3_nodes2 = 1\n",
        "        self.theta12 = tf.Variable(tf.random.normal([features2+1,hidden1_nodes2], dtype= tf.float64) ,name = \"Theta12\")\n",
        "        self.theta22 = tf.Variable(tf.random.normal([hidden1_nodes2+1,hidden2_nodes2], dtype= tf.float64), name = \"Theta22\")\n",
        "        self.theta32 = tf.Variable(tf.random.normal([hidden2_nodes2+1,hidden3_nodes2], dtype= tf.float64), name = \"Theta32\")\n",
        "\n",
        "    def __call__(self, x_train2): # Son metodos que vienen predifinidos desde el object.\n",
        "        bias12 = tf.constant(1, shape=(65317,1), dtype=tf.float64, name='bias12')\n",
        "        bias22 = tf.constant(1, shape=(65317,1), dtype=tf.float64, name='bias22')\n",
        "        bias32 = tf.constant(1, shape=(65317,1), dtype=tf.float64, name='bias32')\n",
        "        a02 = tf.concat([bias12,x_train2],1, name='a0')\n",
        "        z12 = tf.matmul(a02,self.theta12, name='z12') # [4x2]\n",
        "        a12 = tf.concat([bias22,tf.sigmoid(z12)],1,name='a12')\n",
        "        z22 = tf.matmul(a12,self.theta22, name='z22')\n",
        "        a22 = tf.concat([bias32,tf.sigmoid(z22)],1,name=\"a22\")\n",
        "        z32 = tf.matmul(a22,self.theta32, name=\"z32\")\n",
        "        a32 = tf.sigmoid(z32, name='a32')\n",
        "        return a32 # Respuesta, aqui ya predice\n",
        "\n",
        "def loss2(target_y2, predicted_y2): # Funcion de divergencia, Aqui esta la funcion Logistic_regression\n",
        "        regularizacion2=1*tf.reduce_sum(tf.abs(target_y2 - predicted_y2)) \n",
        "        losses2=-tf.reduce_sum(target_y2*tf.math.log(predicted_y2)+(1-target_y2)*tf.math.log(1-predicted_y2),axis=0, name='Cost_function') +regularizacion2\n",
        "        return losses2,regularizacion2\n",
        "\n",
        "def train2(model2, inputs2, outputs2, learning_rate2): # Recibe todo, para entrenar el modelo.\n",
        "    with tf.GradientTape() as t2:\n",
        "        current_loss2 = loss2(outputs2, model2(inputs2)) # El model con el call sabe predecir\n",
        "    dThe12, dThe22, dThe32 = t2.gradient(current_loss2[0], [model2.theta12, model2.theta22, model2.theta32])\n",
        "    model2.theta12.assign_sub(learning_rate2 * dThe12)\n",
        "    model2.theta22.assign_sub(learning_rate2 * dThe22)\n",
        "    model2.theta32.assign_sub(learning_rate2 * dThe32)\n",
        "    \n",
        "def lr_schedule2(epoch2):# se usan rates del orden de 0,0000 porque con rates mayores diverge el problema y entrega loss=Nan\n",
        "    learning_rate2 = 0.0002\n",
        "    if epoch2 > 1500:\n",
        "        learning_rate2 = 0.00009\n",
        "    if epoch2 > 1900:\n",
        "        learning_rate2 = 0.00006\n",
        "    if epoch2 > 3300:\n",
        "        learning_rate2 = 0.00005\n",
        "    if epoch2 > 4800:\n",
        "        learning_rate2 = 0.00002\n",
        "    return learning_rate2\n",
        "\n",
        "def matriz2(y_predicho2,y_test2):\n",
        "    matriz2 = confusion_matrix(y_predicho2, y_test2)\n",
        "    print(matriz2)\n",
        "    return matriz2\n",
        "\n",
        "def evaluar2(X_test2, y_test2, theta1_t2, theta2_t2, theta3_t2, batch_size2):\n",
        "    bias1_t2 = tf.constant(1, shape=(16330,1), dtype=tf.float64, name='bias1_t2')#6292\n",
        "    bias2_t2 = tf.constant(1, shape=(16330,1), dtype=tf.float64, name='bias2_t2')\n",
        "    bias3_t2 = tf.constant(1, shape=(16330,1), dtype=tf.float64, name='bias3_t2')\n",
        "    a0_t2 = tf.concat([bias1_t2, X_test2],1, name='a0_t2')\n",
        "    z1_t2 = tf.matmul(a0_t2,theta1_t2, name='z1_t2') \n",
        "    a1_t2 = tf.concat([bias2_t2, tf.sigmoid(z1_t2)],1,name='a1_t2')\n",
        "    z2_t2 = tf.matmul(a1_t2,theta2_t2, name='z2_t2')\n",
        "    a2_t2 = tf.concat([bias3_t2, tf.sigmoid(z2_t2)],1, name='a2_t2')\n",
        "    z3_t2 = tf.matmul(a2_t2,theta3_t2, name='z3_t2')\n",
        "    a3_t2 = tf.sigmoid(z3_t2, name='a3_t2')\n",
        "    return a3_t2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVrtWIotWWql",
        "colab_type": "text"
      },
      "source": [
        "## Entrenamiento del modelo\n",
        "Ahora podemos llamar a las funciones para entrenar el modelo. Entrenaremos el modelo en un rango de 8000 epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFIOfmvfWW2D",
        "colab_type": "code",
        "outputId": "91107285-79df-450b-c95f-cbf2ab54433d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "model2 = Model2() # Voy ajustando mis tetas en el proceso de aprendizaje\n",
        "theta1_hist2, theta2_hist2, theta3_hist2= [], [], []\n",
        "for epoch2 in range(8000):\n",
        "    current_loss2 = loss2(y_train2, model2(x_train2))\n",
        "    learning_rate2 = lr_schedule2(epoch2)\n",
        "    train2(model2, x_train2,y_train2, learning_rate2=learning_rate2)\n",
        "    if epoch2 % 500== 0:\n",
        "        theta1_hist2.append(model2.theta12.numpy())\n",
        "        theta2_hist2.append(model2.theta22.numpy())\n",
        "        theta3_hist2.append(model2.theta32.numpy())\n",
        "        print('Epoch %2d: learning_rate=%2.5f, loss=%2.5f , regulacion=%2.5f' % (epoch2, learning_rate2, current_loss2[0],current_loss2[1]))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0: learning_rate=0.00020, loss=59254.03285 , regulacion=24868.06892\n",
            "Epoch 500: learning_rate=0.00020, loss=52800.85561 , regulacion=20406.52905\n",
            "Epoch 1000: learning_rate=0.00020, loss=52796.76089 , regulacion=20402.20685\n",
            "Epoch 1500: learning_rate=0.00020, loss=52795.26927 , regulacion=20400.63152\n",
            "Epoch 2000: learning_rate=0.00006, loss=51522.94302 , regulacion=17949.19903\n",
            "Epoch 2500: learning_rate=0.00006, loss=51522.93202 , regulacion=17949.19754\n",
            "Epoch 3000: learning_rate=0.00006, loss=51522.92062 , regulacion=17949.19600\n",
            "Epoch 3500: learning_rate=0.00005, loss=51522.90957 , regulacion=17949.19449\n",
            "Epoch 4000: learning_rate=0.00005, loss=51522.89931 , regulacion=17949.19309\n",
            "Epoch 4500: learning_rate=0.00005, loss=51522.88864 , regulacion=17949.19162\n",
            "Epoch 5000: learning_rate=0.00002, loss=51522.88020 , regulacion=17949.19046\n",
            "Epoch 5500: learning_rate=0.00002, loss=51522.87565 , regulacion=17949.18983\n",
            "Epoch 6000: learning_rate=0.00002, loss=51522.87101 , regulacion=17949.18919\n",
            "Epoch 6500: learning_rate=0.00002, loss=51522.86629 , regulacion=17949.18853\n",
            "Epoch 7000: learning_rate=0.00002, loss=51522.86147 , regulacion=17949.18786\n",
            "Epoch 7500: learning_rate=0.00002, loss=51522.85656 , regulacion=17949.18718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTXCGjH8ZG-i",
        "colab_type": "text"
      },
      "source": [
        "## Evaluación del modelo\n",
        "Finalmente, se realizaron predicciones sobre los datos de entrenamiento para ver qué tan bien los ajustó nuestro modelo durante el entrenamiento. Junto a esto, se midió qué tan bien el modelo realiza generalizaciones con respecto a los datos nuevos. Luego, obtenido la función de perdida, se redondearon los resultados de los datos pronosticados para poder calcular las metricas respectivas de rendimiento y precisión. La matriz de confusión y el f1-score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB-xBSsjZICF",
        "colab_type": "code",
        "outputId": "756c530b-aab1-4274-e227-e736486c7f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "##Evaluacion de datos\n",
        "theta1_t2 = model2.theta12\n",
        "theta2_t2 = model2.theta22\n",
        "theta3_t2 = model2.theta32\n",
        "batch_size2 = 16330\n",
        "\n",
        "#testea con funcion evaluar y redondeo de resultados para evaluar su rendimiento\n",
        "testeo2 = np.round(evaluar2(x_test2,y_test2,theta1_t2,theta2_t2,theta3_t2,batch_size2))\n",
        "X_entrenado2 = np.round(model2(x_train2))\n",
        "\n",
        "#matriz de confusion de entrenamiento\n",
        "print('matriz de confusion de entrenamiento')\n",
        "matriz_train2 = matriz2(X_entrenado2,y_train2)\n",
        "\n",
        "#matriz de confusion de testeo\n",
        "print('matriz de confusion de testeo')\n",
        "matriz_testeo2 = matriz2(testeo2,y_test2)\n",
        "\n",
        "#prueba F1-score\n",
        "significancia_train2 = f1_score(y_train2, X_entrenado2)\n",
        "significancia_testeo2 = f1_score(y_test2, testeo2)\n",
        "print('Precision de entrenamiento= %2.5f'%(significancia_train2))\n",
        "print('Precision de test= %2.5f'% (significancia_testeo2))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "matriz de confusion de entrenamiento\n",
            "[[52467 12850]\n",
            " [    0     0]]\n",
            "matriz de confusion de testeo\n",
            "[[13119  3211]\n",
            " [    0     0]]\n",
            "Precision de entrenamiento= 0.00000\n",
            "Precision de test= 0.00000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}