#%% [markdown]

## Import the required module
#%%
#Declaring necessary modules
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from IPython import display
from matplotlib import cm
from matplotlib import gridspec
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn import metrics
import os
import logging
os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
logging.getLogger("tensorflow").setLevel(logging.WARNING)
import tensorflow as tf
tf.get_logger().setLevel("ERROR")
tf.autograph.set_verbosity(2)
#physical_devices = tf.config.experimental.list_physical_devices('GPU')
#tf.config.experimental.set_memory_growth(physical_devices[0], True)

#%%[markdown]
###variables
#%%
#Importa base de datos de excel
#datos_ingreso_ing=pd.read_excel("C:/Users/56956/Desktop/cosas U/MLBI-master/tareas/dataset11.xlsx")
datos_ingreso_ing.head()
x=np.array(datos_ingreso_ing.drop(['cluster','crr_Nom'],1))
y=np.array(datos_ingreso_ing["cluster"])
print(x,y)
trains=tf.convert_to_tensor(y,tf.float64)
nx=tf.convert_to_tensor(x,tf.float64)
ny=tf.nn.softmax(trains)
print(nx)
tf.shape(nx)



one=tf.convert_to_tensor(np.ones((1,14)),tf.float64)
one1=tf.convert_to_tensor(np.ones((7866,1)),tf.float64)




#%% [markdown]
### Forward Pass
#%%
class Model(object):
    def __init__(self):
    # In practice, these should be initialized to random values (for example, with `tf.random.normal`)
        samples, features = (7866,14) #x.shape
        hidden1_nodes = 14
        hidden2_nodes = 4
        hidden3_nodes=1
        self.theta1 = tf.Variable(tf.random.normal([features+1,hidden1_nodes], dtype= tf.float64) ,name = "Theta1")
        self.theta2 = tf.Variable(tf.random.normal([hidden1_nodes+1,hidden2_nodes], dtype= tf.float64), name = "Theta2")
        self.theta3 = tf.Variable(tf.random.normal([hidden2_nodes+1,hidden3_nodes], dtype= tf.float64), name = "Theta3")

    def __call__(self, x):
        bias1 = one1
        bias2 = one1
        bias3 = one1
        a0 = tf.concat([bias1,nx],1, name='a0')
        z1 = tf.matmul(a0,self.theta1, name='z1')
        a1 = tf.concat([bias2,tf.sigmoid(z1)],1,name='a1')
        z2 = tf.matmul(a1,self.theta2, name='z2')
        a2=tf.concat([bias3,tf.sigmoid(z2)],1,name="a2")
        z3 =tf.matmul(a2,self.theta3, name="z3")
        a3 = tf.sigmoid(z3, name='a3')
        return a3

def loss(target_y, predicted_y):
    return -tf.reduce_sum(target_y*tf.math.log(predicted_y)+(1-target_y)*tf.math.log(1-predicted_y), axis = 0, name='Cost_function')
    
def train(model, inputs, outputs, learning_rate):
    with tf.GradientTape() as t:
        current_loss = loss(outputs, model(inputs))
    dThe1, dThe2, dThe3 = t.gradient(current_loss, [model.theta1, model.theta2, model.theta3])
    model.theta1.assign_sub(learning_rate * dThe1)
    model.theta2.assign_sub(learning_rate * dThe2)
    model.theta3.assign_sub(learning_rate * dThe3)
    
def lr_schedule(epoch):
    """
    Returns a custom learning rate that decreases as epochs progress.
    """
    learning_rate = 0.2
    if epoch > 3000:
        learning_rate = 0.15
    if epoch > 4000:
        learning_rate = 0.1
    if epoch > 4500:
        learning_rate = 0.05
    return learning_rate
#%% [markdown]
### Optimize
    
#%%
model = Model()
theta1_hist, theta2_hist, theta3_hist = [], [], []
for epoch in range(5000):
    current_loss = loss(ny, model(nx))
    learning_rate = lr_schedule(epoch)
    train(model, nx, ny, learning_rate=learning_rate)
    if epoch % 100 == 0:
        theta1_hist.append(model.theta1.numpy())
        theta2_hist.append(model.theta2.numpy())
        theta3_hist.append(model.theta3.numpy())
        print('Epoch %2d: learning_rate=%2.5f, loss=%2.5f' % (epoch, learning_rate, current_loss))


